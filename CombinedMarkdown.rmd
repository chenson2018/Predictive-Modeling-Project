---
title: "CombinedMarkdown"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}    
library(dplyr)
library(tidyr)
library(ggplot2)
library(Amelia)
```

```{r}
#read in merged data, created by conmbining several spreadsheets
Master_Data <- read.csv("master.csv")
```

```{r}
##CLEAN DATA

#Omit NA data from dataframe since very little NA
Master_Data <- na.omit(Master_Data)

#Remove columns we dont need in our analysis
Master_Data$StandardHours <- NULL
Master_Data$Over18 <- NULL
Master_Data$EmployeeCount <- NULL
Master_Data$EmployeeID <- NULL
```

```{r}
##SPLIT DATA
library(caTools)
set.seed(1)
sample <- sample.split(Master_Data$Attrition,SplitRatio = 0.7)

#Training Data
train <- subset(Master_Data,sample==T)
#Testing Data
test <- subset(Master_Data,sample==F)
```

```{r}
##TRAIN THE LOGISTIC REGRESSION MODEL
logModel <- glm(Attrition ~., family = binomial('logit'),data = train)
summary(logModel)
```

```{r}
##PREDICTIONS
pred_probabilities <- predict(logModel,test,type = 'response')
pred_results <- ifelse(pred_probabilities>0.5,1,0)

#Convert Attrition column in test to 0s and 1s to compare to pred_results
test$AttritionClass <- ifelse(test$Attrition == 'Yes',1,0)

misClassError <- mean(pred_results != test$AttritionClass)
accuracy <- (1-misClassError)

cat("Misclassification error:", misClassError, "\n")
cat("Accuracy:", accuracy, "\n")
```

```{r}
#error types
match <- data.frame(cbind(pred_results, reg_results = test$AttritionClass))

match$error[(match$pred_results == 1 & match$reg_results ==1)] <- 'True positive'
match$error[(match$pred_results == 0 & match$reg_results ==1)] <- 'False negative'
match$error[(match$pred_results == 0 & match$reg_results ==0)] <- 'True negative'
match$error[(match$pred_results == 1 & match$reg_results ==0)] <- 'False positive'

errorType <- as.data.frame(addmargins(table(match$error)))

print(errorType)

cat("Probability that an employee predicted to stay leaves:", errorType[1, 2]/(errorType[1, 2] + errorType[3, 2]))
cat("\nProbability that an employee predicted to leave stays:", errorType[2, 2]/(errorType[2, 2] + errorType[4, 2]))
```

```{r}
#LASSO to determine variables to include
library(glmnet)
trainMatrix <- model.matrix(Attrition ~ ., data = train)
testMatrix <- model.matrix(Attrition ~ ., data = test)
grid <- 10 ^ seq(4, -2, length = 100)
lasso <- glmnet(trainMatrix, train$Attrition, alpha = 1, lambda = grid, thresh = 1e-12, family = "binomial")
lassoCV <- cv.glmnet(trainMatrix, train$Attrition, alpha = 1, lambda = grid, thresh = 1e-12, family = "binomial")
lassoLambdaMin <- lassoCV$lambda.min
predict(lasso, s = lassoLambdaMin, type = "coefficients")
```

```{r}
plot(lasso, xvar = "dev")
plot(lasso, xvar = "lambda")
plot(lasso, xvar = "norm")
```

```{r}
#new Logistic model
logModel2 <- glm(Attrition ~ Age + BusinessTravel + JobRole + MaritalStatus + NumCompaniesWorked + StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + YearsSinceLastPromotion + YearsWithCurrManager + EnvironmentSatisfaction + JobSatisfaction + WorkLifeBalance + Avg_Daily_Hours, family = binomial('logit'),data = train)

summary(logModel2)
```

```{r}
pred_probabilities2 <- predict(logModel2,test,type = 'response')
pred_results2 <- ifelse(pred_probabilities2>0.5,1,0)

#Convert Attrition column in test to 0s and 1s to compare to pred_results
test$AttritionClass <- ifelse(test$Attrition == 'Yes',1,0)

misClassError2 <- mean(pred_results2 != test$AttritionClass)
accuracy2 <- (1-misClassError2)

cat("Misclassification error:", misClassError2, "\n")
cat("Accuracy:", accuracy2, "\n")
```

```{r}
#error types for new logisitc regression
match2 <- data.frame(cbind(pred_results2, reg_results = test$AttritionClass))

match2$error[(match2$pred_results == 1 & match2$reg_results ==1)] <- 'True positive'
match2$error[(match2$pred_results == 0 & match2$reg_results ==1)] <- 'False negative'
match2$error[(match2$pred_results == 0 & match2$reg_results ==0)] <- 'True negative'
match2$error[(match2$pred_results == 1 & match2$reg_results ==0)] <- 'False positive'

errorType <- as.data.frame(addmargins(table(match2$error)))

print(errorType)

cat("Probability that an employee predicted to stay leaves:", errorType[1, 2]/(errorType[1, 2] + errorType[3, 2]))
cat("\nProbability that an employee predicted to leave stays:", errorType[2, 2]/(errorType[2, 2] + errorType[4, 2]))
```

```{r}
library(randomForest)
set.seed(1)
finrf = randomForest(Attrition~.,data=train,ntree=500)
finrfpred=predict(finrf,newdata=test)


cat("Misclassification error:", sum(abs(as.numeric(test$Attrition)-as.numeric(finrfpred)))/nrow(test))

finrfrmse = sqrt(sum((as.numeric(test$Attrition)-as.numeric(finrfpred))^2)/nrow(test))

varImpPlot(finrf)
```

```{r}
#error types for random forest
forestMatch <- data.frame(cbind(test$Attrition, finrfpred))
names(forestMatch) <- c('actual', 'prediction')

forestMatch$error[(forestMatch$prediction == 2 & forestMatch$actual ==2)] <- 'True positive'
forestMatch$error[(forestMatch$prediction == 1 & forestMatch$actual ==2)] <- 'False negative'
forestMatch$error[(forestMatch$prediction == 1 & forestMatch$actual ==1)] <- 'True negative'
forestMatch$error[(forestMatch$prediction == 2 & forestMatch$actual ==1)] <- 'False positive'

errorType <- as.data.frame(addmargins(table(forestMatch$error)))

print(errorType)

cat("Probability that an employee predicted to stay leaves:", errorType[1, 2]/(errorType[1, 2] + errorType[3, 2]))
cat("\nProbability that an employee predicted to leave stays:", errorType[2, 2]/(errorType[2, 2] + errorType[4, 2]))
```
